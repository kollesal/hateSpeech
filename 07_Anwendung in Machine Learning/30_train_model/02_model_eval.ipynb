{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "I want to find out which model is the best for my data. For this I will check different models and compare them with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of required libraries\n",
    "Those libraries are used in the notebook to perform the data preparation. Maybee you need to install them first with `pip install <library>`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oliver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "As first step I will import the cleaned dataset from the previous notebook \"01_data_prep\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('../Projectwork-master/20_model_evaluation/20_cleaned_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "I want to use 30% of the data for testing and 70% for training. With \"random_state\" I can ensure that the split is always the same and can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Oliver\\Desktop\\Projectwork-master\\02_model_eval.ipynb Zelle 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Oliver/Desktop/Projectwork-master/02_model_eval.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train,X_test,y_train,y_test \u001b[39m=\u001b[39m train_test_split(x,y,test_size\u001b[39m=\u001b[39m\u001b[39m0.30\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code print('Training:', X_train.shape) outputs the number of rows and columns of the training data set X_train, while the code print('Testing:', X_test.shape) outputs the number of rows and columns of the testing data set X_test. These should be in the ratio 30% to 70%. These outputs help to understand the size of the training and testing datasets and ensure that the split has been done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Oliver\\Desktop\\Projectwork-master\\02_model_eval.ipynb Zelle 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Oliver/Desktop/Projectwork-master/02_model_eval.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining:\u001b[39m\u001b[39m'\u001b[39m,X_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Oliver/Desktop/Projectwork-master/02_model_eval.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTesting:\u001b[39m\u001b[39m'\u001b[39m,X_test\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training:',X_train.shape)\n",
    "print('Testing:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training:',y_train.shape)\n",
    "print('Testing:',y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf_Vectorizer\n",
    "For the model evaluation, I use the TfidfVectorizer class from the Scikit-learn library to convert text data into numerical feature vectors. Tfidf stands for \"Term Frequency-Inverse Document Frequency\" and is a technique for weighting words in a text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=50,max_features=10000)\n",
    "vector =  vectorizer.fit_transform(X_train)\n",
    "x_train=vector.toarray()\n",
    "vector2 = vectorizer.transform(X_test)\n",
    "x_test=vector2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_classif,k=7)\n",
    "x_train = selector.fit_transform(x_train,y_train)\n",
    "x_test = selector.fit_transform(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Let's now find out which model is the best for this data. I will compare some different models with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: VotingClassifiers with Naive Bayes\n",
    "The VotingClassifier with Naive Bayes can be used for various NLP tasks, such as sentiment analysis, text classification or speech recognition. By combining the predictions of multiple Naive Bayes classifiers, it can improve performance and achieve more robust predictions. It is important to note that the effectiveness of the VotingClassifier depends on the diversity of the individual Naive Bayes classifiers, i.e. they should consider different features or different hyperparameter configurations to achieve a better aggregated prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb2 = BernoulliNB()\n",
    "nb3 = MultinomialNB()\n",
    "VotingClassifiers = VotingClassifier(estimators=[('GaussianNB', nb),('BernoulliNB',nb2), ('MultinomialNB', nb3)], voting = 'soft')\n",
    "VotingClassifiers.fit(x_train, y_train)\n",
    "VotingClassifiers.score(x_train,y_train),VotingClassifiers.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingclassifier = (VotingClassifiers.score(x_train,y_train),(VotingClassifiers.score(x_test,y_test)))\n",
    "votingclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act=y_test\n",
    "y_pred=VotingClassifiers.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='summer')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree classifier\n",
    "The function of the Decision Tree Classifier is to learn a decision tree from the given training data that can be used to classify new data. This is achieved by the algorithm determining the best feature partitioning based on certain criteria in order to divide the data into subgroups that are as homogeneous as possible. During training, the decision tree learns how to optimally use the features to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifier(criterion='gini',splitter='random',min_samples_leaf=70,max_depth=4,random_state=0)\n",
    "model2.fit(x_train, y_train)\n",
    "print(model2.score(x_train, y_train))\n",
    "print(model2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree_classifier = (model2.score(x_train,y_train),(model2.score(x_test,y_test)))\n",
    "Decision_Tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act=y_test\n",
    "y_pred=model2.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='PiYG')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: RandomizedSearchCV RandomForest\n",
    "The function of RandomizedSearchCV is to evaluate a given number of random combinations of hyperparameters and evaluate the performance of the RandomForest model using these settings. This involves cross-validation, where the training data is split into multiple parts and different combinations of hyperparameters are trained on the partial datasets and tested on a validation dataset. This allows for a robust evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = RandomizedSearchCV(RandomForestClassifier(),{'n_estimators':[4,5],'criterion':['entropy'],\n",
    "                                                      'max_depth':range(1,4),'min_samples_split':range(2,5)},random_state=12)\n",
    "classifiers.fit(x_train, y_train)\n",
    "print('Training score:',classifiers.score(x_train, y_train))\n",
    "print('Testing score:',classifiers.score(x_test,y_test))\n",
    "print(classifiers.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_forest = (classifiers.score(x_train,y_train),(classifiers.score(x_test,y_test)))\n",
    "Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act=y_test\n",
    "y_pred=classifiers.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='Spectral')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: GradientBoosting Classifier\n",
    "The GradientBoosting Classifier is a machine learning algorithm used to classify data. It is based on the idea of gradient boosting, where weak learning algorithms, in this case decision trees, are combined into a strong learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomizedSearchCV(GradientBoostingClassifier(),{\"learning_rate\": range(3,5),\n",
    "                \"max_depth\":[200],\"max_features\":range(6,10,2),\n",
    "                 \"n_estimators\":[10]},random_state=8,n_jobs=-1)\n",
    "model3.fit(x_train,y_train)\n",
    "print('Training score:',model3.score(x_train,y_train))\n",
    "print('Testing score:',model3.score(x_test,y_test))\n",
    "model3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graident_boost = (model3.score(x_train,y_train),(model3.score(x_test,y_test)))\n",
    "Graident_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act=y_test\n",
    "y_pred=model3.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='PRGn')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: XGBoost classifier\n",
    "The function of the XGBoost Classifier is to create an ensemble classifier consisting of a sequence of decision trees. Similar to the GradientBoosting Classifier, the XGBoost Classifier focuses on improving the errors of the previous tree. However, it uses an improved method to calculate the weights of the decision trees and optimise the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier( eval_metric='map',max_depth=200,n_estimators=70,learning_rate=1.99)\n",
    "model.fit(x_train,y_train.replace({\"Real\":0,'Fake':1}))\n",
    "print('Training score:',model.score(x_train,y_train.replace({\"Real\":0,'Fake':1})))\n",
    "print('Testing score:',model.score(x_test,y_test.replace({\"Real\":0,'Fake':1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGboost_classifier = (model.score(x_train,y_train.replace({\"Real\":0,'Fake':1})),model.score(x_test,y_test.replace({\"Real\":0,'Fake':1})))\n",
    "XGboost_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act = y_test.replace({\"Real\":0,'Fake':1})\n",
    "y_pred = model.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='Spectral')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Ada_Boost(Random Forest)\n",
    "The function of the AdaBoost algorithm is to create a sequence of decision trees, each tree based on the errors of the previous tree. The algorithm adjusts the weight of each training example to focus on the examples that were previously misclassified. By iteratively combining the decision trees, a strong classifier is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(criterion='entropy', max_depth=3, min_samples_split=5,\n",
    "                        n_estimators=5)\n",
    "AddaBoosts = AdaBoostClassifier(base_estimator =RandomForest,n_estimators=70,learning_rate=2.38,random_state=1)\n",
    "AddaBoosts.fit(x_train,y_train)\n",
    "print('Training score:',AddaBoosts.score(x_train,y_train))\n",
    "print('Testing score:',AddaBoosts.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_rf = (AddaBoosts.score(x_train,y_train),(AddaBoosts.score(x_test,y_test)))\n",
    "ab_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act = y_test\n",
    "y_pred = AddaBoosts.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='PRGn')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: AddaBoost(Decision Tree)\n",
    "Typically, the AdaBoost algorithm (Adaptive Boosting) is combined with various weak learning algorithms such as decision trees, linear models or other classifiers to create an ensemble model. The algorithm adjusts the weight of each training example to focus on the misclassified examples and iteratively improve the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_tree = DecisionTreeClassifier(max_depth=200, min_samples_leaf=13, random_state=1)\n",
    "AdaBoost = AdaBoostClassifier(base_estimator = Decision_tree,n_estimators=70,learning_rate=2,random_state=1)\n",
    "AdaBoost.fit(x_train, y_train)\n",
    "print('Training score:',AdaBoost.score(x_train, y_train))\n",
    "print('Testing score:',AdaBoost.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaboost_DT = (AdaBoost.score(x_train,y_train),(AdaBoost.score(x_test,y_test)))\n",
    "Adaboost_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act = y_test\n",
    "y_pred = AdaBoost.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='PiYG')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: RandomizedSearch_KNeighborsClassifier\n",
    "The KNeighborsClassifier is a classification algorithm based on the concept of k-nearest neighbours. It assigns a class to a test data point by looking at the k nearest training data points and selecting the most frequent class among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomizedSearchCV(KNeighborsClassifier(),{'n_neighbors':[10],'metric':['manhattan','minkowski','cosine','tanimoto'],\n",
    "                                                   'p':[1,2]},random_state=8,n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "print('Training score:',model.score(x_train, y_train))\n",
    "print('Testing score:',model.score(x_test,y_test))\n",
    "print(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomizedSearch_KNeighborsClassifier = (model.score(x_train,y_train),(model.score(x_test,y_test)))\n",
    "RandomizedSearch_KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "y_act = y_test\n",
    "y_pred = model.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(y_act,y_pred),annot=True,cmap='summer')\n",
    "print(classification_report(y_act,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy DataFrame\n",
    "Now let's compare the accuracy of the different models. For this I will create a dataframe with the accuracy of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_data = {'votingclassifier_NB': votingclassifier,\n",
    "              'DecisionTreeClassifier': Decision_Tree_classifier,\n",
    "              'Random_forest RandomSearchCV': Random_forest,\n",
    "              'Graident_boost RandomSearchCV': Graident_boost,\n",
    "              'RandomizedSearch_KNeighborsClassifier':RandomizedSearch_KNeighborsClassifier,\n",
    "              'Addaboost(Decision Tree)':Adaboost_DT,\n",
    "              'XGboost_classifier':XGboost_classifier,\n",
    "              'AddaBoost(Random Forest)':ab_rf}\n",
    "score_df = pd.DataFrame(score_data).T\n",
    "score_df.rename(columns = {0:'Train score',1:'Test score'}, inplace = True)\n",
    "score_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
